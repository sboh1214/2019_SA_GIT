{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import News List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947708 News Imported\n",
      "947708 News will be used\n",
      "Maximum Bias : 2558.1106672816877\n",
      "Minimum Bias : -4816.148545482104\n",
      "Calibrating Data\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'Data/NewsData_0_1000000fix'\n",
    "last_index = 10000000\n",
    "\n",
    "from data import NewsList\n",
    "\n",
    "def news_calibrate(news_list):\n",
    "    for news in news_list:\n",
    "        if news.Bias > 300:\n",
    "            news.Bias = 300\n",
    "        if news.Bias < -300:\n",
    "            news.Bias = -300\n",
    "        news.Bias = news.Bias / 150\n",
    "    for news in news_list:\n",
    "        for index, bias in enumerate(news.Sentence_Bias):\n",
    "            if bias > 300:\n",
    "                news.Sentence_Bias[index] = 300\n",
    "            if bias < -300:\n",
    "                news.Sentence_Bias[index] = -300\n",
    "            news.Sentence_Bias[index] = bias / 150\n",
    "    return news_list\n",
    "\n",
    "news_list = NewsList().importPickle(file_name)\n",
    "print(str(len(news_list)) + ' News Imported')\n",
    "news_list = news_list[:last_index]\n",
    "print(str(len(news_list)) + ' News will be used')\n",
    "bias = [i.Bias for i in news_list]\n",
    "print('Maximum Bias : ' + str(max(bias)))\n",
    "print('Minimum Bias : ' + str(min(bias)))\n",
    "print('Calibrating Data')\n",
    "news_list = news_calibrate(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def pad(a, max_len):\n",
    "    if len(a) == max_len:\n",
    "        return a\n",
    "    elif len(a) > max_len:\n",
    "        return a[0:max_len]\n",
    "    else:\n",
    "        a.extend([0 for _ in range(max_len - len(a))])\n",
    "        return a\n",
    "\n",
    "rnn_x = []\n",
    "rnn_y = []\n",
    "for news in tqdm(news_list):\n",
    "    rnn_x.extend(news.Content)\n",
    "    rnn_y.extend(news.Sentence_Bias)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(rnn_x)\n",
    "rnn_x_list = tokenizer.texts_to_sequences(rnn_x)\n",
    "rnn_x_array = array([pad(i, max_length) for i in rnn_x_list])\n",
    "rnn_x = rnn_x_array\n",
    "rnn_y = array(rnn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Maximum Sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1713/947708 [00:00<00:55, 17127.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Sentences Count : 613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 947708/947708 [01:17<00:00, 12239.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from math import ceil, sqrt\n",
    "from numpy import array\n",
    "\n",
    "def square(a, side):\n",
    "    try:\n",
    "        avg = sum(a) / len(a)\n",
    "    except ZeroDivisionError:\n",
    "        avg = 0\n",
    "    output = [[avg] * side for _ in range(side)]\n",
    "    for i, bias in enumerate(a):\n",
    "        output[i // side][i % side] = bias\n",
    "    return output\n",
    "\n",
    "print('Count Maximum Sentence')\n",
    "max_sentence = 0\n",
    "for i in news_list:\n",
    "    if max_sentence < len(i.Content):\n",
    "        max_sentence = len(i.Content)\n",
    "cnn_side = ceil(sqrt(max_sentence))\n",
    "print('Maximum Sentences Count : ' + str(max_sentence))\n",
    "\n",
    "cnn_x = []\n",
    "cnn_y = []\n",
    "for news in tqdm(news_list):\n",
    "    cnn_x.append(square(news.Sentence_Bias, cnn_side))\n",
    "    cnn_y.append(news.Bias)\n",
    "\n",
    "cnn_x = array(cnn_x)\n",
    "cnn_x = cnn_x.reshape((len(cnn_x), cnn_side, cnn_side, 1))\n",
    "cnn_y = array(cnn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rnn_epoch = 1\n",
    "rnn_batch = 64\n",
    "rnn_max_features = 100000\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "def rms(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(diff)))\n",
    "\n",
    "rnn_model:Sequential = Sequential([\n",
    "    Input(shape=(max_length,)),\n",
    "    Embedding(rnn_max_features, 100),\n",
    "    Bidirectional(LSTM(100, return_sequences=False)),\n",
    "    Dense(units=1),\n",
    "    Dropout(rate=0.2),\n",
    "])\n",
    "rnn_model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[binary_accuracy, rms])\n",
    "rnn_history = rnn_model.fit(rnn_x, rnn_y,\n",
    "                            batch_size=rnn_batch, epochs=rnn_epoch, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cnn_epoch = 100\n",
    "cnn_batch = 128\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.activations import tanh\n",
    "\n",
    "def rms(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(diff)))\n",
    "\n",
    "cnn_model:Sequential = Sequential([\n",
    "    Input((cnn_side, cnn_side, 1)),\n",
    "    Conv2D(filters=2, kernel_size=(2, 2), activation=tanh),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(rate=0.2),\n",
    "    Conv2D(filters=2, kernel_size=(2, 2), activation=tanh),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(rate=0.2),\n",
    "    Flatten(),\n",
    "    Dense(units=1),\n",
    "    Dropout(rate=0.2),\n",
    "])\n",
    "cnn_model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=1), metrics=[binary_accuracy, rms])\n",
    "cnn_history = cnn_model.fit(cnn_x, cnn_y,\n",
    "                            batch_size=cnn_batch, epochs=cnn_epoch, validation_split=0.2)\n",
    "\n",
    "data = DataFrame(cnn_history.history)\n",
    "date = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "with open(\"cnn-history-\" + date + \".csv\", mode='w') as f:\n",
    "    data.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Predict')\n",
    "import numpy as np\n",
    "\n",
    "PredictX = [814219, 42159, 141318, 48937, 248414]\n",
    "PredictY = []\n",
    "for item in PredictX:\n",
    "    cnn_input = np.expand_dims(np.array(cnn_x[item]), axis=0)\n",
    "    PredictY.append(cnn_model.predict(cnn_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from os import makedirs\n",
    "\n",
    "fig, ((rnn_loss, cnn_loss), (rnn_acc, cnn_acc)) = plt.subplots(nrows=2, ncols=2, constrained_layout=True)\n",
    "\n",
    "rnn_loss.plot(rnn_history.history['binary_accuracy'], 'y', label='train bin acc')\n",
    "rnn_loss.plot(rnn_history.history['val_binary_accuracy'], 'r', label='val bin acc')\n",
    "rnn_acc.plot(rnn_history.history['rms'], 'b', label='train rms')\n",
    "rnn_acc.plot(rnn_history.history['val_rms'], 'g', label='val rms')\n",
    "cnn_loss.plot(cnn_history.history['binary_accuracy'], 'y', label='train bin acc')\n",
    "cnn_loss.plot(cnn_history.history['val_binary_accuracy'], 'r', label='val bin acc')\n",
    "cnn_acc.plot(cnn_history.history['rms'], 'b', label='train rms')\n",
    "cnn_acc.plot(cnn_history.history['val_rms'], 'g', label='val rms')\n",
    "\n",
    "rnn_loss.set_xlabel('epoch')\n",
    "rnn_acc.set_xlabel('epoch')\n",
    "cnn_loss.set_xlabel('epoch')\n",
    "cnn_acc.set_xlabel('epoch')\n",
    "\n",
    "rnn_loss.set_ylabel('loss')\n",
    "rnn_acc.set_ylabel('RMSD')\n",
    "cnn_loss.set_ylabel('loss')\n",
    "cnn_acc.set_ylabel('RMSD')\n",
    "\n",
    "rnn_loss.title.set_text('RNN Loss')\n",
    "rnn_acc.title.set_text('RNN RMSD')\n",
    "cnn_loss.title.set_text('CNN Loss')\n",
    "cnn_acc.title.set_text('CNN RMSD')\n",
    "\n",
    "\n",
    "\n",
    "n = str(datetime.now())\n",
    "makedirs('./result/' + n)\n",
    "\n",
    "Fig.savefig('./result/' + n + '/plot.png', dpi=1000)\n",
    "\n",
    "rnn_model.save('./result/' + n + '/rnn_model.h5')\n",
    "cnn_model.save('./result/' + n + '/cnn_model.h5')\n",
    "\n",
    "history = [rnn_history.history['loss'],\n",
    "            rnn_history.history['val_loss'],\n",
    "            rnn_history.history['binary_accuracy'],\n",
    "            rnn_history.history['val_binary_accuracy'],\n",
    "            rnn_history.history['rms'],\n",
    "            rnn_history.history['val_rms'],\n",
    "            cnn_history.history['loss'],\n",
    "            cnn_history.history['val_loss'],\n",
    "            cnn_history.history['binary_accuracy'],\n",
    "            cnn_history.history['val_binary_accuracy'],\n",
    "            cnn_history.history['rms'],\n",
    "            cnn_history.history['val_rms']]\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
