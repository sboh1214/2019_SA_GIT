{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import News List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'Data/NewsData_0_1000000fix'\n",
    "last_index = 10000000\n",
    "\n",
    "from data import NewsList\n",
    "\n",
    "def news_calibrate(news_list):\n",
    "    for news in news_list:\n",
    "        if news.Bias > 300:\n",
    "            news.Bias = 300\n",
    "        if news.Bias < -300:\n",
    "            news.Bias = -300\n",
    "        news.Bias = news.Bias / 150\n",
    "    for news in news_list:\n",
    "        for index, bias in enumerate(news.Sentence_Bias):\n",
    "            if bias > 300:\n",
    "                news.Sentence_Bias[index] = 300\n",
    "            if bias < -300:\n",
    "                news.Sentence_Bias[index] = -300\n",
    "            news.Sentence_Bias[index] = bias / 150\n",
    "    return news_list\n",
    "\n",
    "news_list = NewsList().importPickle(file_name)\n",
    "print(str(len(news_list)) + ' News Imported')\n",
    "news_list = news_list[:last_index]\n",
    "print(str(len(news_list)) + ' News will be used')\n",
    "bias = [i.Bias for i in news_list]\n",
    "print('Maximum Bias : ' + str(max(bias)))\n",
    "print('Minimum Bias : ' + str(min(bias)))\n",
    "print('Calibrating Data')\n",
    "news_list = news_calibrate(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def pad(a, max_len):\n",
    "    if len(a) == max_len:\n",
    "        return a\n",
    "    elif len(a) > max_len:\n",
    "        return a[0:max_len]\n",
    "    else:\n",
    "        a.extend([0 for _ in range(max_len - len(a))])\n",
    "        return a\n",
    "\n",
    "rnn_x = []\n",
    "rnn_y = []\n",
    "for news in tqdm(news_list):\n",
    "    rnn_x.extend(news.Content)\n",
    "    rnn_y.extend(news.Sentence_Bias)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(rnn_x)\n",
    "rnn_x_list = tokenizer.texts_to_sequences(rnn_x)\n",
    "rnn_x_array = array([pad(i, max_length) for i in rnn_x_list])\n",
    "rnn_x = rnn_x_array\n",
    "rnn_y = array(rnn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from math import ceil, sqrt\n",
    "from numpy import array\n",
    "\n",
    "def square(a, side):\n",
    "    try:\n",
    "        avg = sum(a) / len(a)\n",
    "    except ZeroDivisionError:\n",
    "        avg = 0\n",
    "    output = [[avg] * side for _ in range(side)]\n",
    "    for i, bias in enumerate(a):\n",
    "        output[i // side][i % side] = bias\n",
    "    return output\n",
    "\n",
    "print('Count Maximum Sentence')\n",
    "max_sentence = 0\n",
    "for i in news_list:\n",
    "    if max_sentence < len(i.Content):\n",
    "        max_sentence = len(i.Content)\n",
    "cnn_side = ceil(sqrt(max_sentence))\n",
    "print('Maximum Sentences Count : ' + str(max_sentence))\n",
    "\n",
    "cnn_x = []\n",
    "cnn_y = []\n",
    "for news in tqdm(news_list):\n",
    "    cnn_x.append(square(news.Sentence_Bias, cnn_side))\n",
    "    cnn_y.append(news.Bias)\n",
    "\n",
    "cnn_x = array(cnn_x)\n",
    "cnn_x = cnn_x.reshape((len(cnn_x), cnn_side, cnn_side, 1))\n",
    "cnn_y = array(cnn_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Run RNN Model')\n",
    "rnn_epoch = 1\n",
    "rnn_batch = 64\n",
    "rnn_max_features = 100000\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "def rms(y_true, y_pred):\n",
    "    diff = y_true - y_pred\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(diff)))\n",
    "\n",
    "rnn_model:Sequential = Sequential([\n",
    "    Input(shape=(max_length,)),\n",
    "    Embedding(rnn_max_features, 100),\n",
    "    Bidirectional(LSTM(100, return_sequences=False)),\n",
    "    Dropout(rate=0.2),\n",
    "    Dense(units=1),\n",
    "    Dropout(rate=0.2),\n",
    "])\n",
    "rnn_model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[binary_accuracy, rms])\n",
    "rnn_history = rnn_model.fit(rnn_x, rnn_y,\n",
    "                            batch_size=rnn_batch, epochs=rnn_epoch, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Run CNN Model')\n",
    "cnn_epoch = 10\n",
    "cnn_batch = 64\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "cnn_model:Sequential = Sequential([\n",
    "    Input((cnn_side, cnn_side, 1)),\n",
    "    Conv2D(filters=2, kernel_size=(2, 2)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(rate=0.2),\n",
    "    Flatten(),\n",
    "    Dense(units=1),\n",
    "    Dropout(rate=0.2),\n",
    "])\n",
    "cnn_model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[binary_accuracy, rms])\n",
    "cnn_history = cnn_model.fit(cnn_x, cnn_y,\n",
    "                            batch_size=cnn_batch, epochs=cnn_epoch, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Predict')\n",
    "import numpy as np\n",
    "\n",
    "PredictX = [814219, 42159, 141318, 48937, 248414]\n",
    "PredictY = []\n",
    "for item in PredictX:\n",
    "    cnn_input = np.expand_dims(np.array(cnn_x[item]), axis=0)\n",
    "    PredictY.append(cnn_model.predict(cnn_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from os import makedirs\n",
    "\n",
    "fig, ((rnn_loss, cnn_loss), (rnn_acc, cnn_acc)) = plt.subplots(nrows=2, ncols=2, constrained_layout=True)\n",
    "\n",
    "rnn_loss.plot(rnn_history.history['binary_accuracy'], 'y', label='train bin acc')\n",
    "rnn_loss.plot(rnn_history.history['val_binary_accuracy'], 'r', label='val bin acc')\n",
    "rnn_acc.plot(rnn_history.history['rms'], 'b', label='train rms')\n",
    "rnn_acc.plot(rnn_history.history['val_rms'], 'g', label='val rms')\n",
    "cnn_loss.plot(cnn_history.history['binary_accuracy'], 'y', label='train bin acc')\n",
    "cnn_loss.plot(cnn_history.history['val_binary_accuracy'], 'r', label='val bin acc')\n",
    "cnn_acc.plot(cnn_history.history['rms'], 'b', label='train rms')\n",
    "cnn_acc.plot(cnn_history.history['val_rms'], 'g', label='val rms')\n",
    "\n",
    "rnn_loss.set_xlabel('epoch')\n",
    "rnn_acc.set_xlabel('epoch')\n",
    "cnn_loss.set_xlabel('epoch')\n",
    "cnn_acc.set_xlabel('epoch')\n",
    "\n",
    "rnn_loss.set_ylabel('loss')\n",
    "rnn_acc.set_ylabel('RMSD')\n",
    "cnn_loss.set_ylabel('loss')\n",
    "cnn_acc.set_ylabel('RMSD')\n",
    "\n",
    "rnn_loss.title.set_text('RNN Loss')\n",
    "rnn_acc.title.set_text('RNN RMSD')\n",
    "cnn_loss.title.set_text('CNN Loss')\n",
    "cnn_acc.title.set_text('CNN RMSD')\n",
    "\n",
    "\n",
    "\n",
    "n = str(datetime.now())\n",
    "makedirs('./result/' + n)\n",
    "\n",
    "Fig.savefig('./result/' + n + '/plot.png', dpi=1000)\n",
    "\n",
    "rnn_model.save('./result/' + n + '/rnn_model.h5')\n",
    "cnn_model.save('./result/' + n + '/cnn_model.h5')\n",
    "\n",
    "history = [rnn_history.history['loss'],\n",
    "            rnn_history.history['val_loss'],\n",
    "            rnn_history.history['binary_accuracy'],\n",
    "            rnn_history.history['val_binary_accuracy'],\n",
    "            rnn_history.history['rms'],\n",
    "            rnn_history.history['val_rms'],\n",
    "            cnn_history.history['loss'],\n",
    "            cnn_history.history['val_loss'],\n",
    "            cnn_history.history['binary_accuracy'],\n",
    "            cnn_history.history['val_binary_accuracy'],\n",
    "            cnn_history.history['rms'],\n",
    "            cnn_history.history['val_rms']]\n",
    "\n",
    "from csv import writer\n",
    "\n",
    "with open('./result/' + n + '/history.csv', mode='w') as f:\n",
    "    csv = writer(f)\n",
    "    for line in history:\n",
    "        csv.writerow(line)\n",
    "\n",
    "with open('./result/' + n + '/predict.csv', mode='w') as f:\n",
    "    csv = writer(f)\n",
    "    for line in PredictY:\n",
    "        csv.writerow(line)\n",
    "    for index in PredictX:\n",
    "        csv.writerow(news_list[index].Title)\n",
    "    for index in PredictY:\n",
    "        csv.writerow(news_list[index].Bias)\n",
    "\n",
    "basic = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1>Machine Learning Result</h1>\n",
    "<div class=\"datetime\">Date and Time : </div>\n",
    "\n",
    "<h2>Environment</h2>\n",
    "<div class=\"sys_info\">System Info : </div>\n",
    "<div class=\"py_version\">Python Version : </div>\n",
    "<div class=\"keras_backend\">Keras Backend : </div>\n",
    "\n",
    "<h2>RNN Configuration</h2>\n",
    "<div class=\"rnn_epoch\">Epoch : </div>\n",
    "<div class=\"rnn_batch\">Batch Size : </div>\n",
    "<div class=\"rnn_model\"></div>\n",
    "\n",
    "<h2>CNN Configuration</h2>\n",
    "<div class=\"cnn_epoch\">Epoch : </div>\n",
    "<div class=\"cnn_batch\">Batch Size : </div>\n",
    "<div class=\"cnn_model\"></div>\n",
    "\n",
    "<h2>Plot<h2>\n",
    "<img src=\"plot.png\" alt=\"plt\" height=\"400\" width=\"600\">\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(basic, 'html.parser')\n",
    "soup.html.body.find('div', attrs={'class': 'datetime'}).append(str(n))\n",
    "soup.html.body.find('div', attrs={'class': 'sys_info'}).append(str(platform()))\n",
    "soup.html.body.find('div', attrs={'class': 'py_version'}).append(str(version_info))\n",
    "\n",
    "soup.html.body.find('div', attrs={'class': 'rnn_epoch'}).append(str(rnn_epoch))\n",
    "soup.html.body.find('div', attrs={'class': 'rnn_batch'}).append(str(rnn_batch))\n",
    "Rnn.summary(print_fn=lambda x: to_html(soup, 'rnn_model', x))\n",
    "\n",
    "soup.html.body.find('div', attrs={'class': 'cnn_epoch'}).append(str(cnn_epoch))\n",
    "soup.html.body.find('div', attrs={'class': 'cnn_batch'}).append(str(cnn_batch))\n",
    "Rnn.summary(print_fn=lambda x: to_html(soup, 'cnn_model', x))\n",
    "\n",
    "soup.prettify()\n",
    "with open('./result/' + n + '/result.html', mode='w') as f:\n",
    "    f.write(str(soup))\n",
    "\n",
    "def to_html(soup: BeautifulSoup, class_: str, x: str):\n",
    "    soup.html.body.find('div', attrs={'class': class_}).append(x)\n",
    "    soup.html.body.find('div', attrs={'class': class_}).append(soup.new_tag('br'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
